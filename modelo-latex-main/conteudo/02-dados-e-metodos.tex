%!TeX root=../tese.tex
%("dica" para o editor de texto: este arquivo Ã© parte de um documento maior)
% para saber mais: https://tex.stackexchange.com/q/78101

\chapter{Data and Methods}
\label{chap:install}

This chapter explains how the experiment was carried out and what steps were followed to compare different embedding models. Our main goal is to understand whether replacing the embedding model in \textcite{pilone2024deepermatcher} affects the final similarity results.

This project follows a replication and extension approach. The original DeeperMatcher code and logic were kept as close as possible to the previous implementation, with the only intentional modification being the embedding models used to generate vector representations. By isolating this variable, it becomes possible to observe how much different embedding models actually influence the outcome.

\section{Dataset}
The dataset used in this work originates from the study of \textcite{pilone2024deepermatcher}, which investigated the use of text embeddings to match issues and review comments from software projects. In that study, the author conducted a evaluation using issues and reviews from 20 different open-source projects, over 87000 issues and  1270000 reviews. This material is suitable for embedding analysis because the issues and reviews contain detailed natural language descriptions that can be transformed into meaningful vector representations.

The dataset includes:
\begin{itemize}
    \item issue descriptions from these projects,
    \item user reviews collected from Google Play Store,
    \item relevance feedback from programmers, who rated the top matches generated by DeeperMatcher.
\end{itemize}

In addition to collecting textual data, \textcite{pilone2024deepermatcher} created a survey evaluation to generate human labels for a subset of issue-review pairs that were matches according to DeepMatcher. Selected pairs were presented to programmers, who were asked first to decide whether the review was relevant to the issue (True or False), and then to rate the degree of relevance on a scale from 1 to 5. This survey was conducted on 19 projects, totalizing 4000 feedbacks over 400 issues, and 2000 issue-review pairs. There are more than 4000 feedbacks because part of the issues were reviewed three times. Each suggestion was reviewed twice, and some (where there was disagreement) were reviewed three times. The final score was the average of the ratings. These evaluations produced a valuable human-validated subset of the data, indicating which matches were genuinely meaningful according to practitioners

Since the dataset had already been preprocessed by the original researchers, no additional cleaning or normalization was applied. Using the data exactly as provided ensures that results remain directly comparable to the original work. This dataset is appropriate for the present work because its organization ensures that issues and reviews are kept within their respective projects. The large number of review texts provides sufficient content for computing embeddings and examining similarity patterns across different models. Furthermore, the human-validated relevance scores from the forms experiment allow a direct comparison between the rankings produced by alternative embedding models and those considered relevant by programmers. Each suggestion was reviewed twice, and some (where there was disagreement) were reviewed three times. The final score was the average of the ratings.

\section{Project Selection}

For this study, we restricted ourselves to a limited subset of the same projects used during the field study conducted in \textcite{pilone2024deepermatcher}, where programmers evaluated the quality of the generated matches. Using these projects maintains consistency with the original evaluation setup and enables a direct comparison against human relevance scores. The following projects were selected:

\begin{itemize}
      \item \textbf{WordPress} (\texttt{wordpress\_android}) --- a content management system for websites and blogs, available on Google Play Store\footnote{\url{https://play.google.com/store/apps/details?id=org.wordpress.android}}.
      \item \textbf{K9 Mail} (\texttt{fsck\_k9}) --- an email client application, available on Google Play Store\footnote{\url{https://play.google.com/store/apps/details?id=com.fsck.k9}}.
      \item \textbf{PPSSPP} (\texttt{ppsspp\_ppsspp}) --- a PlayStation Portable emulator, available on Google Play Store\footnote{\url{https://play.google.com/store/apps/details?id=org.ppsspp.ppsspp}}.
      \item \textbf{Mindustry} (\texttt{anuke\_mindustry}) --- a tower defense and factory management game, available on Google Play Store\footnote{\url{https://play.google.com/store/apps/details?id=io.anuke.mindustry}}.
\end{itemize}

These projecrs have between 12177 (K9 Mail) and 104652 (WordPress) reviews, and between 331 (PPSSPP) and 8758 (WordPress) issues.

\section{Embedding Models}

This study evaluates the performance of several widely used embedding models in semantic similarity tasks involving issues and review comments. The work of \textcite{pilone2024deepermatcher} used the Jina 3  (jina-embeddings-v3), so this model was also included to maintain compatibility with the original experiment. The Jina model is an open-source, Transformer-based embedding model executed locally.

To compare Jina with models from different providers, additional embedding models were included. Unlike Jina, these models are proprietary and accessed through API services, but they are commonly used in industry and research due to their strong performance across semantic tasks. The models evaluated in this study were:

\begin{itemize}
    \item OpenAI Large --- a higher-capacity embedding model designed for tasks requiring more detailed semantic representations.
    \item OpenAI Small --- a lighter and faster variant, suitable for applications where efficiency is more important than maximum representation depth.
    \item Gemini Text Embedding 004 --- Google's current-generation embedding model, optimized for semantic similarity and retrieval tasks.
    \item Cohere Embed v4.0 --- Cohere's latest embedding model, designed for high-quality text encoding across multilingual and domain-diverse inputs.
\end{itemize}

These models were selected because they represent different families of widely used embedding models. Their comparison allows an analysis of whether different models produce similar similarity rankings when applied to the same project-level data.

\section{Execution and Code Adaptation}

To extend the original system, the existing codebase was modified to integrate multiple embedding models. Because some models run locally while others depend on remote API calls, the pipeline was rewritten to use Python's asynchronous programming. This allowed embeddings to be generated in parallel, reducing total execution time and preventing API latency from slowing down the process.

No fine-tuning or parameter adjustment was applied. All embeddings were generated using the default settings of each model.

The full original DeeperMatcher pipeline was used as the foundation of this project. Only improvements related to organization and embedding integration were added, such as:
\begin{itemize}
    \item modularizing the code,
    \item adding asynchronous execution,
    \item creating adapters for each embedding model,
    \item extending the evaluation workflow to handle multiple models.
\end{itemize}

All core logic remained unchanged. This ensures that differences in results can be attributed to the embeddings themselves and not to modifications in the experimental method.

\section{Similarity Computation}

The similarity score between an issue and a review was calculated using normalized cosine similarity, following exactly the same definition used in DeeperMatcher.

After all issues and reviews were embedded with the chosen model, for each issue:
\begin{itemize}
    \item similarity scores were computed against all reviews in the project,
    \item the results were sorted in descending order,
    \item the top 10 most similar reviews were selected.
\end{itemize}

The top-10 ranking was preserved from the original research to guarantee consistency with the research of \textcite{pilone2024deepermatcher}.

These ranked results serve as the basis for:
\begin{itemize}
    \item comparing different embedding models, and
    \item checking how similarity scores align with the human-assigned relevance levels (1 to 5).
\end{itemize}

\section{Comparison Against Human Feedback}

Although the original relevance labels come from the previous study, they remain essential to evaluating embedding quality. In this work, similarity values from each embedding model were compared across relevance levels to investigate whether:
\begin{itemize}
    \item higher relevance reviews tend to have higher cosine similarity,
    \item different models show similar or different behavioral patterns,
    \item any embedding model correlates more strongly with human judgment.
\end{itemize}

This provides a practical measure of how ``semantically accurate'' each model is in real software engineering scenarios.

\section{Clusterization and Visualization}

To explore how each embedding model organizes semantic space, vector representations were projected into two dimensions using t-SNE (t-Distributed Stochastic Neighbor Embedding), and the clusterization was performed on reviews and issues.

The purpose of these visualizations is to inspect:
\begin{itemize}
    \item whether issues and reviews cluster distinctly or overlap,
    \item how the cluster layout differs between embedding models,
    \item whether cluster structures match the similarity and relevance patterns observed.
\end{itemize}

This qualitative analysis helps complement the numerical results by showing how each model structures meaning in high-dimensional space.
